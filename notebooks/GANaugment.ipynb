{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNCBYbrkRRiBhm8jZLqrDWh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OrionXV/Volve-Well-Log-Prediction/blob/main/notebooks/GANaugment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GAN Augmentation\n",
        "\n",
        "Predictors used:\n",
        "\n",
        "* Decsion Tree\n",
        "* Gradient Booster\n",
        "* Neural Network\n",
        "* LSTM\n"
      ],
      "metadata": {
        "id": "7M9HNcHs-bDR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zLaLeNT8vcaM",
        "outputId": "a65480e3-e797-4bdc-8e2e-11d68f667ba0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: lasio in /usr/local/lib/python3.10/dist-packages (0.31)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from lasio) (1.25.2)\n",
            "Requirement already satisfied: md_toc in /usr/local/lib/python3.10/dist-packages (9.0.0)\n",
            "Requirement already satisfied: fpyutils<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from md_toc) (4.0.1)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.25.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.11.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.36.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.62.2)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n",
            "Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.10/dist-packages (0.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons) (24.0)\n",
            "Requirement already satisfied: typeguard<3.0.0,>=2.7 in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons) (2.13.3)\n",
            "Requirement already satisfied: smogn in /usr/local/lib/python3.10/dist-packages (0.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from smogn) (1.25.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from smogn) (2.0.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from smogn) (4.66.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->smogn) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->smogn) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->smogn) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->smogn) (1.16.0)\n",
            "Collecting tensorflow-gpu\n",
            "  Downloading tensorflow-gpu-2.12.0.tar.gz (2.6 kB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        }
      ],
      "source": [
        "!pip install lasio\n",
        "!pip install md_toc\n",
        "!pip install tensorflow\n",
        "!pip install tensorflow-addons\n",
        "!pip install smogn\n",
        "!pip install tensorflow-gpu\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import itertools\n",
        "\n",
        "import lasio\n",
        "\n",
        "import glob\n",
        "import os\n",
        "import md_toc\n",
        "\n",
        "from sklearn.svm import OneClassSVM\n",
        "\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import PowerTransformer\n",
        "\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.layers import LSTM, Bidirectional\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "from keras import layers\n",
        "from keras import models\n",
        "\n",
        "import tensorflow as tf\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "5qYaMOe7v0uJ"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
      ],
      "metadata": {
        "id": "zJGEsSvHN-Qq",
        "outputId": "ccb68f05-5e0b-40d7-c278-6c8aee220725",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num GPUs Available:  0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fetching Data"
      ],
      "metadata": {
        "id": "iigc0Zqb_BEr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/OrionXV/Volve-Well-Log-Prediction.git"
      ],
      "metadata": {
        "id": "01jw9w2Xv3w2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find paths to the log files (MS windows path style)\n",
        "paths = sorted(glob.glob(os.path.join(os.getcwd(),\"/content/Volve-Well-Log-Prediction/well_logs\", \"*.LAS\")))\n",
        "\n",
        "# Create a list for loop processing\n",
        "log_list = [0] * len(paths)\n",
        "\n",
        "# Parse LAS with LASIO to create pandas df\n",
        "for i in range(len(paths)):\n",
        "  df = lasio.read(paths[i])\n",
        "  log_list[i] = df.df()\n",
        "  # this transforms the depth from index to regular column\n",
        "  log_list[i].reset_index(inplace=True)\n",
        "\n",
        "log_list[0].head()"
      ],
      "metadata": {
        "id": "-77u3egEv6Fu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepareing Data"
      ],
      "metadata": {
        "id": "N1QgQdJa_HW1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save logs from list of dfs into separate variables\n",
        "log1, log2, log3, log4, log5 = log_list\n",
        "# Lists of depths for clipping\n",
        "lower = [2600, 3200, 2620, 3100, 3100]\n",
        "upper = [3720, 4740, 3640, 3400, 4050]\n",
        "\n",
        "# Lists of selected columns\n",
        "train_cols = ['DEPTH', 'NPHI', 'RHOB', 'GR', 'RT', 'PEF', 'CALI', 'DT']\n",
        "test_cols = ['DEPTH', 'NPHI', 'RHOB', 'GR', 'RT', 'PEF', 'CALI']\n",
        "\n",
        "log_list_clipped = [0] * len(paths)\n",
        "\n",
        "for i in range(len(log_list)):\n",
        "\n",
        "  # Clip depths\n",
        "  temp_df = log_list[i].loc[\n",
        "      (log_list[i]['DEPTH'] >= lower[i]) &\n",
        "      (log_list[i]['DEPTH'] <= upper[i])\n",
        "  ]\n",
        "\n",
        "  # Select train-log columns\n",
        "  if i in [0,2,3]:\n",
        "    log_list_clipped[i] = temp_df[train_cols]\n",
        "\n",
        "  # Select test-log columns\n",
        "  else:\n",
        "    log_list_clipped[i] = temp_df[test_cols]\n",
        "\n",
        "# Save logs from list into separate variables\n",
        "log1, log2, log3, log4, log5 = log_list_clipped\n",
        "# check for NaN\n",
        "log1.head()"
      ],
      "metadata": {
        "id": "nJ9TvGN2v-qW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Concatenate dataframes\n",
        "train = pd.concat([log1, log3, log4])\n",
        "pred = pd.concat([log2, log5])\n",
        "\n",
        "# Assign names\n",
        "names = ['15_9-F-11A', '15_9-F-11B', '15_9-F-1A', '15_9-F-1B', '15_9-F-1C']\n",
        "\n",
        "names_train = []\n",
        "names_pred = []\n",
        "\n",
        "for i in range(len(log_list_clipped)):\n",
        "  if i in [0,2,3]:\n",
        "    # Train data, assign names\n",
        "    names_train.append(np.full(len(log_list_clipped[i]), names[i]))\n",
        "  else:\n",
        "    # Test data, assign names\n",
        "    names_pred.append(np.full(len(log_list_clipped[i]), names[i]))\n",
        "\n",
        "# Concatenate inside list\n",
        "names_train = list(itertools.chain.from_iterable(names_train))\n",
        "names_pred = list(itertools.chain.from_iterable(names_pred))\n",
        "\n",
        "# Add well name to df\n",
        "train['WELL'] = names_train\n",
        "pred['WELL'] = names_pred\n",
        "\n",
        "# Pop and add depth to end of df\n",
        "depth_train, depth_pred = train.pop('DEPTH'), pred.pop('DEPTH')\n",
        "train['DEPTH'], pred['DEPTH'] = depth_train, depth_pred\n",
        "\n",
        "# Train dataframe with logs 1,3,4 vertically stacked\n",
        "train.head()"
      ],
      "metadata": {
        "id": "4GMpFFXPwIGO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "colnames = train.columns\n",
        "only_feature = ['NPHI', 'RHOB', 'GR', 'RT', 'PEF', 'CALI'] # only feature column names\n",
        "only_target = 'DT' # only target column names\n",
        "feature_target = np.append(only_feature, only_target) # feature and target column names\n",
        "\n",
        "colnames"
      ],
      "metadata": {
        "id": "gksTNy6M5tPA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Transformation and Normalization"
      ],
      "metadata": {
        "id": "7X3_wWUP_L6c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Log transform the RT to logarithmic\n",
        "train['RT'] = np.log10(train['RT'])\n",
        "\n",
        "# Transformation / Normalizer object Yeo-Johnson method\n",
        "scaler = PowerTransformer(method='yeo-johnson')\n",
        "\n",
        "# ColumnTransformer (feature_target defines to which it is applied, leave Well and Depth untouched)\n",
        "ct = ColumnTransformer([('transform', scaler, feature_target)], remainder='passthrough')\n",
        "\n",
        "# Fit and transform\n",
        "train_trans = ct.fit_transform(train)\n",
        "\n",
        "# Convert to dataframe\n",
        "train_trans = pd.DataFrame(train_trans, columns=colnames)\n",
        "train_trans.head()"
      ],
      "metadata": {
        "id": "XBUyMK9L4YbQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Outlier Removal"
      ],
      "metadata": {
        "id": "dwqkieLT_Qe1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a copy of train\n",
        "train_fonly = train_trans.copy()\n",
        "\n",
        "# Remove WELL, DEPTH\n",
        "train_fonly = train_fonly.drop(['WELL', 'DEPTH'], axis=1)\n",
        "train_fonly_names = train_fonly.columns\n",
        "\n",
        "# Helper function for repeated plotting\n",
        "\n",
        "def makeboxplot(my_title='enter title',my_data=None):\n",
        "    _, ax1 = plt.subplots()\n",
        "    ax1.set_title(my_title, size=15)\n",
        "    ax1.boxplot(my_data)\n",
        "    ax1.set_xticklabels(train_fonly_names)\n",
        "    plt.show()\n",
        "\n",
        "makeboxplot('Unprocessed',train_trans[train_fonly_names])\n",
        "print('n samples unprocessed:', len(train_fonly))"
      ],
      "metadata": {
        "id": "b2bUHhni4epB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "svm = OneClassSVM(nu=0.1)\n",
        "yhat = svm.fit_predict(train_fonly)\n",
        "mask = yhat != -1\n",
        "train_svm = train_fonly[mask]\n",
        "\n",
        "makeboxplot('Processed via Support Vector Machine',train_svm)\n",
        "print('Remaining samples:', len(train_svm))"
      ],
      "metadata": {
        "id": "RD5l64WJwPPL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_names = ['NPHI', 'RHOB', 'GR', 'RT', 'PEF', 'CALI']\n",
        "target_name = 'DT'\n"
      ],
      "metadata": {
        "id": "YmEezCa16WR5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train and Validation Prep"
      ],
      "metadata": {
        "id": "CJPn54Vk_XI_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select columns for features (X) and target (y)\n",
        "X_train = train_svm[feature_names].values.astype('float32')\n",
        "y_train = train_svm[target_name].values.reshape(-1, 1).astype('float32')\n",
        "\n",
        "# Define the validation data\n",
        "train_trans_copy = train_trans.copy()\n",
        "\n",
        "train_well_names = ['15_9-F-11A', '15_9-F-1A', '15_9-F-1B']\n",
        "\n",
        "X_val = []\n",
        "y_val = []\n",
        "\n",
        "for i in range(len(train_well_names)):\n",
        "    # Split the df by log name\n",
        "    val = train_trans_copy.loc[train_trans_copy['WELL'] == train_well_names[i]].copy()\n",
        "\n",
        "    # Drop name column\n",
        "    val.drop(['WELL'], axis=1, inplace=True)\n",
        "\n",
        "    # Define X_val (feature) and y_val (target) as NumPy arrays\n",
        "    X_val_ = val[feature_names].values.astype('float32')\n",
        "    y_val_ = val[target_name].values.reshape(-1, 1).astype('float32')\n",
        "\n",
        "    X_val.append(X_val_)\n",
        "    y_val.append(y_val_)\n",
        "\n",
        "# Save into separate NumPy arrays\n",
        "X_val1, X_val3, X_val4 = X_val\n",
        "y_val1, y_val3, y_val4 = y_val"
      ],
      "metadata": {
        "id": "LSyrjgy04lF_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = X_train.astype('float32')\n",
        "\n",
        "# Cast y_train to float32\n",
        "y_train = y_train.astype('float32')\n",
        "\n",
        "# Cast X_val1, X_val3, X_val4 to float32\n",
        "X_val1 = X_val1.astype('float32')\n",
        "X_val3 = X_val3.astype('float32')\n",
        "X_val4 = X_val4.astype('float32')\n",
        "\n",
        "# Cast y_val1, y_val3, y_val4 to float32\n",
        "y_val1 = y_val1.astype('float32')\n",
        "y_val3 = y_val3.astype('float32')\n",
        "y_val4 = y_val4.astype('float32')"
      ],
      "metadata": {
        "id": "0Cpqdhw6292K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Augmentation"
      ],
      "metadata": {
        "id": "o0C6_VYcFnDy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select columns for features (X) and target (y)\n",
        "train_gen = pd.merge(train_svm, train_trans, on = ['NPHI', 'RHOB', 'GR', 'RT', 'PEF', 'CALI', 'DT'], how='left')"
      ],
      "metadata": {
        "id": "-KvXyPeiUx-L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_gen.head()"
      ],
      "metadata": {
        "id": "Pe5l41PeVwbh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_features = len(train_gen.drop('WELL', axis = 1).columns)"
      ],
      "metadata": {
        "id": "GpG9L81w-nsA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_well_names = ['15_9-F-11A', '15_9-F-1A', '15_9-F-1B']\n",
        "\n",
        "df_gen1 = train_gen[train_gen['WELL'] == '15_9-F-11A'].copy()\n",
        "df_gen3 = train_gen[train_gen['WELL'] == '15_9-F-1A'].copy()\n",
        "df_gen4 = train_gen[train_gen['WELL'] == '15_9-F-1B'].copy()"
      ],
      "metadata": {
        "id": "dpYj3M0ILOXi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_gen1['DEPTH'] = scaler.fit_transform(df_gen1['DEPTH'])\n",
        "df_gen3['DEPTH'] = scaler.fit_transform(df_gen3['DEPTH'])\n",
        "df_gen4['DEPTH'] = scaler.fit_transform(df_gen4['DEPTH'])"
      ],
      "metadata": {
        "id": "eQmz8Uf3DYsD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the GAN model architecture\n",
        "def build_generator(latent_dim, num_features):\n",
        "    model = models.Sequential()\n",
        "    model.add(layers.Dense(128, input_dim=latent_dim, activation=None))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.Activation('relu'))\n",
        "    model.add(layers.Dense(128, activation=None))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.Activation('relu'))\n",
        "    model.add(layers.Dense(256, activation=None))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.Activation('relu'))\n",
        "    model.add(layers.Dense(num_features, activation='linear'))  # Output layer\n",
        "    return model\n",
        "\n",
        "def build_discriminator(num_features):\n",
        "    model = models.Sequential()\n",
        "    model.add(layers.Dense(256, input_shape=(num_features,), activation=None))\n",
        "    model.add(layers.LeakyReLU(alpha=0.2))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.Dense(128, activation=None))\n",
        "    model.add(layers.LeakyReLU(alpha=0.2))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.Dense(128, activation=None))\n",
        "    model.add(layers.LeakyReLU(alpha=0.2))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.Dense(1, activation='sigmoid'))  # Output layer\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "v3snZrXvF-7B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator_loss_fn =  tf.keras.losses.BinaryCrossentropy()\n",
        "discriminator_loss =  tf.keras.losses.BinaryCrossentropy()\n",
        "\n",
        "generator_optimizer = tf.keras.optimizers.Adam(learning_rate=3e-4)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(learning_rate=3e-4)"
      ],
      "metadata": {
        "id": "Fw9cnbSn-RJJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_gan(generator, discriminator, epochs, batch_size, data_frame):\n",
        "    data_frame = data_frame\n",
        "\n",
        "    # Determine the number of batches\n",
        "    num_samples = len(data_frame)\n",
        "    num_batches = num_samples // batch_size\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        avg_gen_loss = 0\n",
        "        avg_disc_loss = 0\n",
        "        with tqdm(total=num_batches, desc=f'Epoch {epoch+1}/{epochs}', unit='batch') as pbar:\n",
        "\n",
        "            for batch in range(num_batches):\n",
        "\n",
        "                # Get real data for the current batch\n",
        "                batch_start = batch * batch_size\n",
        "                batch_end = (batch + 1) * batch_size\n",
        "                real_data = data_frame.iloc[batch_start:batch_end]\n",
        "                real_data = real_data.values.astype('float32')\n",
        "\n",
        "                # Sample random noise for generator input\n",
        "                noise = np.random.normal(0, 1, size=(len(real_data), latent_dim))\n",
        "\n",
        "                # Generate synthetic data\n",
        "                generated_data = generator.predict(noise,  verbose=0)\n",
        "\n",
        "                # Combine real and generated data\n",
        "                x_combined = np.concatenate([real_data, generated_data])\n",
        "\n",
        "                # Labels for real and generated data\n",
        "                y_real = np.ones((batch_size, 1))\n",
        "                y_generated = np.zeros((batch_size, 1))\n",
        "                y_combined = np.concatenate([y_real, y_generated])\n",
        "\n",
        "                # Train discriminator\n",
        "                with tf.GradientTape() as disc_tape:\n",
        "                    # Forward pass: compute logits for real and fake samples\n",
        "                    real_logits = discriminator(real_data, training=True)\n",
        "                    fake_logits = discriminator(generated_data, training=True)\n",
        "\n",
        "                    # Compute discriminator loss\n",
        "                    real_labels = tf.ones_like(real_logits)\n",
        "                    fake_labels = tf.zeros_like(fake_logits)\n",
        "                    discriminator_loss_real = discriminator_loss(real_labels, real_logits)\n",
        "                    discriminator_loss_fake = discriminator_loss(fake_labels, fake_logits)\n",
        "                    discriminator_loss_total = (discriminator_loss_real + discriminator_loss_fake)\n",
        "\n",
        "                # Compute gradients of discriminator loss with respect to discriminator trainable variables\n",
        "                gradients_of_discriminator = disc_tape.gradient(discriminator_loss_total, discriminator.trainable_variables)\n",
        "\n",
        "                # Update discriminator weights using optimizer\n",
        "                discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
        "\n",
        "                with tf.GradientTape() as gen_tape:\n",
        "                    # Generate fake samples using the generator\n",
        "                    fake_samples = generator(noise, training=True)\n",
        "\n",
        "                    # Compute generator loss\n",
        "                    generator_loss = generator_loss_fn(real_labels, discriminator(fake_samples, training=True))\n",
        "\n",
        "                # Compute gradients of generator loss with respect to generator trainable variables\n",
        "                gradients_of_generator = gen_tape.gradient(generator_loss, generator.trainable_variables)\n",
        "\n",
        "                # Update generator weights using optimizer\n",
        "                generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
        "\n",
        "                avg_disc_loss += discriminator_loss_total\n",
        "                avg_gen_loss +=  generator_loss\n",
        "\n",
        "                pbar.update(1)\n",
        "\n",
        "            # Print progress\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Discriminator Loss: {avg_disc_loss/num_batches}, Generator Loss: {avg_gen_loss/num_batches}\")\n"
      ],
      "metadata": {
        "id": "5pizWjSP-Tmk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate synthetic data for certain depths\n",
        "def generate_synthetic_data(generator, num_samples):\n",
        "    noise = np.random.normal(0, 1, size=(num_samples, latent_dim))\n",
        "    synthetic_data = generator.predict(noise)\n",
        "    return synthetic_data\n"
      ],
      "metadata": {
        "id": "65H6rdje-c9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bym3H5Y7IfaZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the GAN for log 1\n",
        "# Define GAN components\n",
        "latent_dim = 128\n",
        "generator_1 = build_generator(latent_dim, num_features)\n",
        "discriminator_1 = build_discriminator(num_features)\n",
        "\n",
        "epochs = 32\n",
        "batch_size = 32\n",
        "num_batches = len(df_gen1) // batch_size\n",
        "train_gan(generator_1, discriminator_1, epochs, batch_size, df_gen1.drop(\"WELL\", axis = 1))"
      ],
      "metadata": {
        "id": "W-nyGJpQ-cUH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "latent_dim = 128\n",
        "\n",
        "generator_3 = build_generator(latent_dim, num_features)\n",
        "discriminator_3 = build_discriminator(num_features)\n",
        "\n",
        "epochs = 32\n",
        "batch_size = 32\n",
        "num_batches = len(df_gen3) // batch_size\n",
        "\n",
        "train_gan(generator_3, discriminator_3, epochs, batch_size, df_gen3.drop(\"WELL\", axis = 1))"
      ],
      "metadata": {
        "id": "2ZWZizDKIKbs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "latent_dim = 128\n",
        "\n",
        "generator_4 = build_generator(latent_dim, num_features)\n",
        "discriminator_4 = build_discriminator(num_features)\n",
        "\n",
        "epochs = 32\n",
        "batch_size = 32\n",
        "num_batches = len(df_gen4) // batch_size\n",
        "\n",
        "train_gan(generator_4, discriminator_4, epochs, batch_size, df_gen4.drop(\"WELL\", axis = 1))"
      ],
      "metadata": {
        "id": "krKgMTZnILDj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage: generate synthetic data for 100 samples\n",
        "synthetic_data_1 = generate_synthetic_data(generator_1, int(len(df_gen1)*0.33))"
      ],
      "metadata": {
        "id": "jqNcXVFG-f2f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage: generate synthetic data for 100 samples\n",
        "synthetic_data_3 = generate_synthetic_data(generator_3, int(len(df_gen1)*0.33))"
      ],
      "metadata": {
        "id": "yVtQq9yrIMwq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage: generate synthetic data for 100 samples\n",
        "synthetic_data_4 = generate_synthetic_data(generator_4, int(len(df_gen1)*0.33))"
      ],
      "metadata": {
        "id": "nMILs6uWIN-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "columns = ['NPHI', 'RHOB', 'GR', 'RT', 'PEF', 'CALI', 'DT', 'DEPTH']\n",
        "\n",
        "synthetic_df_1 = pd.DataFrame(synthetic_data_1, columns=columns)\n",
        "synthetic_df_3 = pd.DataFrame(synthetic_data_3, columns=columns)\n",
        "synthetic_df_4 = pd.DataFrame(synthetic_data_4, columns=columns)\n"
      ],
      "metadata": {
        "id": "JTQ9zttxuYnh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_gen1.drop('WELL', axis = 1). apply(lambda x: x.astype('float32')).describe()"
      ],
      "metadata": {
        "id": "NmQgwjV5uk6I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler.fit(train_gen[train_gen['WELL'] == '15_9-F-11A']['DEPTH'])\n",
        "df_gen1['DEPTH'] = scaler.inverse_transform(df_gen1['DEPTH'].reshape(-1, 1))\n",
        "synthetic_df_1['DEPTH'] = scaler.inverse_transform(synthetic_df_1['DEPTH'].reshape(-1, 1))\n",
        "\n",
        "scaler.fit(train_gen[train_gen['WELL'] == '15_9-F-1A']['DEPTH'])\n",
        "df_gen3['DEPTH'] = scaler.inverse_transform(df_gen3['DEPTH'].reshape(-1, 1))\n",
        "synthetic_df_3['DEPTH'] = scaler.inverse_transform(synthetic_df_3['DEPTH'].reshape(-1, 1))\n",
        "\n",
        "\n",
        "scaler.fit(train_gen[train_gen['WELL'] == '15_9-F-1B']['DEPTH'])\n",
        "df_gen4['DEPTH'] = scaler.inverse_transform(df_gen4['DEPTH'].reshape(-1, 1))\n",
        "synthetic_df_4['DEPTH'] = scaler.inverse_transform(synthetic_df_4['DEPTH'].reshape(-1, 1))\n"
      ],
      "metadata": {
        "id": "5i80YrVOGbGH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "synthetic_df_1.describe()"
      ],
      "metadata": {
        "id": "2YuRirmIue69"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "synthetic_df_1['WELL'] = df_gen1['WELL'][0]\n",
        "synthetic_df_3['WELL'] = df_gen3['WELL'][0]\n",
        "synthetic_df_4['WELL'] = df_gen4['WELL'][0]"
      ],
      "metadata": {
        "id": "OhTmjSBJFCWM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(nrows=len(df_gen1.columns), ncols=2, figsize=(12, 20))\n",
        "fig.subplots_adjust(hspace=0.5)\n",
        "\n",
        "# Plot histograms for each feature in df_gen1 and synthetic_df_1\n",
        "for i, column in enumerate(df_gen1.columns):\n",
        "    sns.histplot(df_gen1[column], ax=axes[i, 0], kde=True, color='blue', label='df_gen1')\n",
        "    axes[i, 0].set_title(f'{column} - df_gen1')\n",
        "    axes[i, 0].legend()\n",
        "\n",
        "    sns.histplot(synthetic_df_1[column], ax=axes[i, 1], kde=True, color='orange', label='synthetic_df_1')\n",
        "    axes[i, 1].set_title(f'{column} - synthetic_df_1')\n",
        "    axes[i, 1].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7mfhnOpQEPW5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(nrows=len(df_gen3.columns), ncols=2, figsize=(12, 20))\n",
        "fig.subplots_adjust(hspace=0.5)\n",
        "\n",
        "# Plot histograms for each feature in df_gen3 and synthetic_df_3\n",
        "for i, column in enumerate(df_gen3.columns):\n",
        "    sns.histplot(df_gen3[column], ax=axes[i, 0], kde=True, color='blue', label='df_gen3')\n",
        "    axes[i, 0].set_title(f'{column} - df_gen3')\n",
        "    axes[i, 0].legend()\n",
        "\n",
        "    sns.histplot(synthetic_df_3[column], ax=axes[i, 1], kde=True, color='orange', label='synthetic_df_3')\n",
        "    axes[i, 1].set_title(f'{column} - synthetic_df_3')\n",
        "    axes[i, 1].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ro9hhf_BLbZi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(nrows=len(df_gen4.columns), ncols=2, figsize=(12, 20))\n",
        "fig.subplots_adjust(hspace=0.5)\n",
        "\n",
        "# Plot histograms for each feature in df_gen1 and synthetic_df_4\n",
        "for i, column in enumerate(df_gen4.columns):\n",
        "    sns.histplot(df_gen4[column], ax=axes[i, 0], kde=True, color='blue', label='df_gen4')\n",
        "    axes[i, 0].set_title(f'{column} - df_gen4')\n",
        "    axes[i, 0].legend()\n",
        "\n",
        "    sns.histplot(synthetic_df_4[column], ax=axes[i, 1], kde=True, color='orange', label='synthetic_df_4')\n",
        "    axes[i, 1].set_title(f'{column} - synthetic_df_4')\n",
        "    axes[i, 1].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6lt_LcdkLcJC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_gen1 = pd.concat([df_gen1, synthetic_df_1])\n",
        "df_gen1 = df_gen1.sort_values(by='DEPTH')\n",
        "\n",
        "df_gen3 = pd.concat([df_gen1, synthetic_df_3])\n",
        "df_gen3  = df_gen3.sort_values(by='DEPTH')\n",
        "\n",
        "df_gen4 = pd.concat([df_gen1, synthetic_df_4])\n",
        "df_gen4 = df_gen4.sort_values(by='DEPTH')\n",
        "\n",
        "\n",
        "X_train_new = pd.concat([df_gen1, df_gen3, df_gen4])\n",
        "y_train_new = X_train_new['DT']\n",
        "X_train_new.drop('DT', axis = 1)"
      ],
      "metadata": {
        "id": "O4iFeYBqGJEC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decison Tree Regressor\n"
      ],
      "metadata": {
        "id": "YsOA0CjmPv1b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Decision Tree Regressor object\n",
        "model_decision_tree = DecisionTreeRegressor()\n",
        "\n",
        "# Fit the regressor to the training data\n",
        "model_decision_tree.fit(X_train_new, y_train_new)\n",
        "\n",
        "# Validation: Predict on well 1\n",
        "y_pred1_decision_tree = model_decision_tree.predict(X_val1)\n",
        "print(\"R2 Log 1: {}\".format(round(model_decision_tree.score(X_val1, y_val1), 4)))\n",
        "rmse = np.sqrt(mean_squared_error(y_val1, y_pred1_decision_tree))\n",
        "print(\"RMSE Log 1: {}\".format(round(rmse, 4)))\n",
        "\n",
        "# Validation: Predict on well 3\n",
        "y_pred3_decision_tree = model_decision_tree.predict(X_val3)\n",
        "print(\"R2 Log 3: {}\".format(round(model_decision_tree.score(X_val3, y_val3), 4)))\n",
        "rmse = np.sqrt(mean_squared_error(y_val3, y_pred3_decision_tree))\n",
        "print(\"RMSE Log 3: {}\".format(round(rmse, 4)))\n",
        "\n",
        "# Validation: Predict on well 4\n",
        "y_pred4_decision_tree = model_decision_tree.predict(X_val4)\n",
        "print(\"R2 Log 4: {}\".format(round(model_decision_tree.score(X_val4, y_val4), 4)))\n",
        "rmse = np.sqrt(mean_squared_error(y_val4, y_pred4_decision_tree))\n",
        "print(\"RMSE Log 4: {}\".format(round(rmse, 4)))\n"
      ],
      "metadata": {
        "id": "CXo45bgRPwS6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gradient Booster Regressor"
      ],
      "metadata": {
        "id": "I7zZseYQ_aJC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradient Booster object\n",
        "model_grad_boost = GradientBoostingRegressor()\n",
        "\n",
        "# Fit the regressor to the training data\n",
        "model_grad_boost.fit(X_train_new, y_train_new)\n",
        "\n",
        "# Validation: Predict on well 1\n",
        "y_pred1_grad_boost = model_grad_boost.predict(X_val1)\n",
        "print(\"R2 Log 1: {}\".format(round(model_grad_boost.score(X_val1, y_val1),4)))\n",
        "rmse = np.sqrt(mean_squared_error(y_val1, y_pred1_grad_boost))\n",
        "print(\"RMSE Log 1: {}\".format(round(rmse,4)))\n",
        "\n",
        "# Validation: Predict on well 3\n",
        "y_pred3_grad_boost = model_grad_boost.predict(X_val3)\n",
        "print(\"R2 Log 3: {}\".format(round(model_grad_boost.score(X_val3, y_val3),4)))\n",
        "rmse = np.sqrt(mean_squared_error(y_val3, y_pred3_grad_boost))\n",
        "print(\"RMSE Log 3: {}\".format(round(rmse,4)))\n",
        "\n",
        "# Validation: Predict on well 4\n",
        "y_pred4_grad_boost = model_grad_boost.predict(X_val4)\n",
        "print(\"R2 Log 4: {}\".format(round(model_grad_boost.score(X_val4, y_val4),4)))\n",
        "rmse = np.sqrt(mean_squared_error(y_val4, y_pred4_grad_boost))\n",
        "print(\"RMSE Log 4: {}\".format(round(rmse,4)))"
      ],
      "metadata": {
        "id": "OMnCQ-j24tAa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Neural Network"
      ],
      "metadata": {
        "id": "uIH7QeL5_d83"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the neural network model\n",
        "def neural_network_model(input_shape):\n",
        "    model = Sequential([\n",
        "        Dense(64, activation='relu', input_shape=input_shape),\n",
        "        Dense(64, activation='relu'),\n",
        "        Dense(64, activation='relu'),\n",
        "        Dense(1)  # Output layer\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Initialize and compile the neural network model\n",
        "model_nn = neural_network_model(X_train_new.shape[1:])\n",
        "model_nn.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Fit the neural network model to the training data\n",
        "history_nn = model_nn.fit(X_train_new, y_train_new, epochs=16, batch_size=32, validation_data=(X_val1, y_val1))\n",
        "\n",
        "# Validation: Predict on well 1\n",
        "y_pred1_nn = model_nn.predict(X_val1)\n",
        "rmse_nn1 = np.sqrt(mean_squared_error(y_val1, y_pred1_nn))\n",
        "print(\"RMSE Neural Network Log 1: {}\".format(round(rmse_nn1, 4)))\n",
        "\n",
        "# Validation: Predict on well 3\n",
        "y_pred3_nn = model_nn.predict(X_val3)\n",
        "rmse_nn3 = np.sqrt(mean_squared_error(y_val3, y_pred3_nn))\n",
        "print(\"RMSE Neural Network Log 3: {}\".format(round(rmse_nn3, 4)))\n",
        "\n",
        "# Validation: Predict on well 4\n",
        "y_pred4_nn = model_nn.predict(X_val4)\n",
        "rmse_nn4 = np.sqrt(mean_squared_error(y_val4, y_pred4_nn))\n",
        "print(\"RMSE Neural Network Log 4: {}\".format(round(rmse_nn4, 4)))\n"
      ],
      "metadata": {
        "id": "XjqW-5Y_vbdv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LSTM"
      ],
      "metadata": {
        "id": "fBGryQ82_jDh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preparing data for LSTM\n",
        "time_steps = 10\n",
        "\n",
        "def prep_for_lstm(X_in, y_in, time_steps = 10):\n",
        "    X_lstm = []\n",
        "    y_lstm = []\n",
        "    for i in range(len(X_in) - time_steps):\n",
        "        X_lstm.append(X_in[i:i+time_steps])\n",
        "        y_lstm.append(y_in[i+time_steps])\n",
        "\n",
        "    X_lstm = np.array(X_lstm)\n",
        "    y_lstm = np.array(y_lstm)\n",
        "    return X_lstm, y_lstm\n",
        "\n",
        "X_train_lstm, y_train_lstm = prep_for_lstm(X_train_new, y_train_new, time_steps = time_steps)\n",
        "X_val1_lstm, y_val1_lstm = prep_for_lstm(X_val1, y_val1, time_steps = time_steps)\n",
        "X_val3_lstm, y_val3_lstm = prep_for_lstm(X_val3, y_val3, time_steps = time_steps)\n",
        "X_val4_lstm, y_val4_lstm = prep_for_lstm(X_val4, y_val4, time_steps = time_steps)\n"
      ],
      "metadata": {
        "id": "fXLVSIuakZmF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_lstm.shape"
      ],
      "metadata": {
        "id": "39PXKrfWnIom"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define bidirectional LSTM model with three layers\n",
        "model_lstm = Sequential()\n",
        "model_lstm.add(Bidirectional(LSTM(units=64, return_sequences=True), input_shape=(X_train_lstm.shape[1], X_train_lstm.shape[2])))\n",
        "model_lstm.add(Bidirectional(LSTM(units=64, return_sequences=True)))\n",
        "model_lstm.add(Bidirectional(LSTM(units=64)))\n",
        "model_lstm.add(Dense(units=1))  # Output layer\n",
        "model_lstm.compile(optimizer=Adam(), loss='mean_squared_error')\n",
        "\n",
        "# Fit the LSTM model to the training data\n",
        "history = model_lstm.fit(X_train_lstm, y_train_lstm, epochs=16, batch_size=32, validation_data=(X_val1_lstm, y_val1_lstm))\n",
        "\n",
        "# Validation: Predict on well 1\n",
        "y_pred1_lstm = model_lstm.predict(X_val1_lstm)\n",
        "rmse_lstm1 = np.sqrt(mean_squared_error(y_val1_lstm, y_pred1_lstm))\n",
        "print(\"RMSE LSTM Log 1: {}\".format(round(rmse_lstm1, 4)))\n",
        "\n",
        "# Validation: Predict on well 3\n",
        "y_pred3_lstm = model_lstm.predict(X_val3_lstm)\n",
        "rmse_lstm3 = np.sqrt(mean_squared_error(y_val3_lstm, y_pred3_lstm))\n",
        "print(\"RMSE LSTM Log 3: {}\".format(round(rmse_lstm3, 4)))\n",
        "\n",
        "# Validation: Predict on well 4\n",
        "y_pred4_lstm = model_lstm.predict(X_val4_lstm)\n",
        "rmse_lstm4 = np.sqrt(mean_squared_error(y_val4_lstm, y_pred4_lstm))\n",
        "print(\"RMSE LSTM Log 4: {}\".format(round(rmse_lstm4, 4)))\n"
      ],
      "metadata": {
        "id": "5MoSpJyhvibp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Formatting back the predictions"
      ],
      "metadata": {
        "id": "YZvpQW36wP9E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make the transformer fit to the target\n",
        "y = train[target_name].values\n",
        "scaler.fit(y.reshape(-1,1))\n",
        "\n",
        "# Inverse transform  y_val, y_pred\n",
        "y_val1 = scaler.inverse_transform(y_val1.reshape(-1,1))\n",
        "y_val3 = scaler.inverse_transform(y_val3.reshape(-1,1))\n",
        "y_val4 = scaler.inverse_transform(y_val4.reshape(-1,1))\n",
        "\n",
        "\n",
        "# Define a list to hold all predictions\n",
        "all_predictions = [y_pred1_grad_boost, y_pred3_grad_boost, y_pred4_grad_boost,\n",
        "                   y_pred1_nn, y_pred3_nn, y_pred4_nn,\n",
        "                   y_pred1_lstm, y_pred3_lstm, y_pred4_lstm,\n",
        "                   y_pred1_decision_tree, y_pred3_decision_tree, y_pred4_decision_tree\n",
        "                   ]\n",
        "\n",
        "# Inverse transform all predictions\n",
        "all_predictions_inverse = [scaler.inverse_transform(pred.reshape(-1, 1)) for pred in all_predictions]\n",
        "\n",
        "# Split the transformed predictions back into separate variables\n",
        "(\n",
        "    y_pred1_grad_boost_inv,  y_pred3_grad_boost_inv, y_pred4_grad_boost_inv,\n",
        "    y_pred1_nn_inv, y_pred3_nn_inv, y_pred4_nn_inv,\n",
        "    y_pred1_lstm_inv, y_pred3_lstm_inv, y_pred4_lstm_inv,\n",
        "    y_pred1_decision_tree_inv, y_pred3_decision_tree_inv, y_pred4_decision_tree_inv\n",
        ") = all_predictions_inverse"
      ],
      "metadata": {
        "id": "ld0qkINW4yPn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_plott = [y_pred1_decision_tree_inv, y_pred1_grad_boost_inv, y_pred1_nn_inv,  y_pred1_lstm_inv,\n",
        "           y_pred3_decision_tree_inv, y_pred3_grad_boost_inv, y_pred3_nn_inv,  y_pred3_lstm_inv,\n",
        "           y_pred4_decision_tree_inv, y_pred4_grad_boost_inv, y_pred4_nn_inv,  y_pred4_lstm_inv]\n",
        "\n",
        "x_real_plott = [\n",
        "    y_val1,\n",
        "    y_val3,\n",
        "    y_val4\n",
        "]\n",
        "\n",
        "x_real_plott_lstm = [\n",
        "    y_val1_lstm,\n",
        "    y_val3_lstm,\n",
        "    y_val4_lstm\n",
        "]\n",
        "\n",
        "y_plott = [log1['DEPTH'],\n",
        "           log3['DEPTH'],\n",
        "           log4['DEPTH']]\n",
        "\n",
        "color_preds = ['green', 'orange', 'blue', 'cyan'] * 3\n",
        "title_preds = ['Pred. DT Log 1 (DTree)', 'Pred. DT Log 1 (Grad Boost)', 'Pred. DT Log 1 (NN)',  'Pred. DT Log 1 (LSTM)',\n",
        "               'Pred. DT Log 3 (DTree)', 'Pred. DT Log 3 (Grad Boost)', 'Pred. DT Log 3 (NN)',  'Pred. DT Log 1 (LSTM)',\n",
        "               'Pred. DT Log 4 (DTree)', 'Pred. DT Log 4 (Grad Boost)', 'Pred. DT Log 4 (NN)',  'Pred. DT Log 1 (LSTM)',]\n",
        "\n",
        "fig, ax = plt.subplots(nrows=1, ncols=12, figsize=(25, 10))\n",
        "\n",
        "for i, (x_pred, title) in enumerate(zip(x_plott, title_preds)):\n",
        "    # Reshape y[i] to match the shape of x_pred\n",
        "    y_i_reshaped = y_plott[i//4]\n",
        "    ax[i].plot(x_pred, y_i_reshaped[:len(x_pred)], color=color_preds[i])\n",
        "    ax[i].plot(x_real_plott[i//4], y_i_reshaped, color='purple', linestyle='dashed', alpha = 0.5) #if i%4 != 0 else x_real_plott_lstm[i//4]\n",
        "    ax[i].set_xlim(50, 150)\n",
        "    ax[i].set_ylim(max(y_i_reshaped), min(y_i_reshaped))  # Ensure correct order of y-axis\n",
        "    ax[i].set_title(title)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "MTqXbeTo43VQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the test data\n",
        "names_test = ['15_9-F-11B', '15_9-F-1C']\n",
        "\n",
        "X_test = []\n",
        "y_test = []\n",
        "depths = []\n",
        "\n",
        "for i in range(len(names_test)):\n",
        "  # split the df with respect to its name\n",
        "  test = pred.loc[pred['WELL'] == names_test[i]]\n",
        "\n",
        "  # Drop well name column\n",
        "  test = test.drop(['WELL'], axis=1)\n",
        "\n",
        "  # Define X_test (feature)\n",
        "  X_test_ = test[feature_names].values\n",
        "\n",
        "  # Define depth\n",
        "  depth_ = test['DEPTH'].values\n",
        "\n",
        "  X_test.append(X_test_)\n",
        "  depths.append(depth_)\n",
        "\n",
        "# For each well 2 and 5\n",
        "X_test2, X_test5 = X_test\n",
        "depth2, depth5 = depths"
      ],
      "metadata": {
        "id": "LKrU4E_r5BAT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test2"
      ],
      "metadata": {
        "id": "e1GeQYBnD8t9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform X_test of log 2 and 5\n",
        "X_test2 = scaler.fit_transform(X_test2)\n",
        "X_test5 = scaler.fit_transform(X_test5)\n",
        "\n",
        "X_test2_lstm, _ = prep_for_lstm(X_test2, X_test2, time_steps = time_steps)\n",
        "X_test5_lstm, _ = prep_for_lstm(X_test5, X_test5, time_steps = time_steps)\n",
        "\n",
        "\n",
        "# Predictions for log 2 using all models\n",
        "y_pred2_grad_boost = model_grad_boost.predict(X_test2)\n",
        "y_pred2_decision_tree = model_decision_tree.predict(X_test2)\n",
        "y_pred2_nn = model_nn.predict(X_test2)\n",
        "y_pred2_lstm = model_lstm.predict(X_test2_lstm)\n",
        "#y_pred2_transformer = model_transformer.predict(X_test2)\n",
        "\n",
        "# Predictions for log 5 using all models\n",
        "y_pred5_grad_boost = model_grad_boost.predict(X_test5)\n",
        "y_pred5_decision_tree = model_decision_tree.predict(X_test5)\n",
        "y_pred5_nn = model_nn.predict(X_test5)\n",
        "y_pred5_lstm = model_lstm.predict(X_test5_lstm)\n",
        "#y_pred5_transformer = model_transformer.predict(X_test5)\n",
        "\n",
        "y = train[target_name].values\n",
        "scaler.fit(y.reshape(-1,1))\n",
        "\n",
        "# Inverse transform y_pred for log 2\n",
        "y_pred2_grad_boost = scaler.inverse_transform(y_pred2_grad_boost.reshape(-1,1))\n",
        "y_pred2_decision_tree = scaler.inverse_transform(y_pred2_decision_tree.reshape(-1,1))\n",
        "y_pred2_nn = scaler.inverse_transform(y_pred2_nn.reshape(-1,1))\n",
        "y_pred2_lstm = scaler.inverse_transform(y_pred2_lstm.reshape(-1,1))\n",
        "#y_pred2_transformer = scaler.inverse_transform(y_pred2_transformer.reshape(-1,1))\n",
        "\n",
        "# Inverse transform y_pred for log 5\n",
        "y_pred5_grad_boost = scaler.inverse_transform(y_pred5_grad_boost.reshape(-1,1))\n",
        "y_pred5_decision_tree = scaler.inverse_transform(y_pred5_decision_tree.reshape(-1,1))\n",
        "y_pred5_nn = scaler.inverse_transform(y_pred5_nn.reshape(-1,1))\n",
        "y_pred5_lstm = scaler.inverse_transform(y_pred5_lstm.reshape(-1,1))\n",
        "#y_pred5_transformer = scaler.inverse_transform(y_pred5_transformer.reshape(-1,1))\n"
      ],
      "metadata": {
        "id": "sUHLhpGs5BjX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pad_predicted_values(predicted_values, target_column, window_size=10):\n",
        "    \"\"\"\n",
        "    Pad the predicted values using a moving average to fill in the gaps.\n",
        "\n",
        "    Parameters:\n",
        "        predicted_values (np.ndarray): The predicted values to pad.\n",
        "        target_column (pd.Series): The target column used to calculate moving average.\n",
        "        window_size (int): The size of the moving average window.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: The padded predicted values.\n",
        "    \"\"\"\n",
        "    # Calculate the number of values to pad\n",
        "    pad_length = len(target_column) - len(predicted_values)\n",
        "\n",
        "    # Pad the predicted values with NaNs to match the length of target_column\n",
        "    padding = np.full((pad_length, 1), np.nan)\n",
        "\n",
        "    padded_values = np.concatenate((padding, predicted_values))\n",
        "\n",
        "    for i in range(len(padded_values)-1, -1, -1):\n",
        "        if np.isnan(padded_values[i][0]):\n",
        "            padded_values[i][0] = np.mean(padded_values[i+1:i+1+window_size])\n",
        "\n",
        "\n",
        "\n",
        "    return padded_values\n"
      ],
      "metadata": {
        "id": "OGr3U8XQ3HTx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred2_lstm"
      ],
      "metadata": {
        "id": "CJxm3uYP4Hrx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pad_predicted_values(y_pred2_lstm, y_pred2_nn)"
      ],
      "metadata": {
        "id": "JJ9yY1-n48YS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add predictions for log 2 to the dataframes\n",
        "log2['DT_grad_boost'] = y_pred2_grad_boost\n",
        "log2['DT_Tree'] = y_pred2_decision_tree\n",
        "log2['DT_nn'] = y_pred2_nn\n",
        "log2['DT_lstm'] = pad_predicted_values(y_pred2_lstm, log2['DT_Tree'])\n",
        "\n",
        "\n",
        "# Add predictions for log 5 to the dataframes\n",
        "log5['DT_grad_boost'] = y_pred5_grad_boost\n",
        "log5['DT_Tree'] = y_pred5_decision_tree\n",
        "log5['DT_nn'] = y_pred5_nn\n",
        "log5['DT_lstm'] = pad_predicted_values(y_pred5_lstm, log5['DT_Tree'])\n",
        "#log5['DT_transformer'] = y_pred5_transformer\n"
      ],
      "metadata": {
        "id": "wvTLiXRX69zI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(18, 12))\n",
        "\n",
        "# Plot predictions for log 2\n",
        "\n",
        "plt.subplot(1, 8, 1)\n",
        "plt.plot(y_pred2_decision_tree, depth2, color='green', label='Decision Tree')\n",
        "plt.ylim(max(depth2), min(depth2))\n",
        "plt.title('Pred DT Log 2: 15_9-F-11B', size=12)\n",
        "plt.xlabel('DT')\n",
        "plt.ylabel('Depth')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 8, 2)\n",
        "plt.plot(y_pred2_grad_boost, depth2, color='blue', label='Gradient Boosting')\n",
        "plt.ylim(max(depth2), min(depth2))\n",
        "plt.title('Pred DT Log 2: 15_9-F-11B', size=12)\n",
        "plt.xlabel('DT')\n",
        "plt.ylabel('Depth')\n",
        "plt.legend()\n",
        "\n",
        "\n",
        "plt.subplot(1, 8, 3)\n",
        "plt.plot(y_pred2_nn, depth2, color='orange', label='Neural Network')\n",
        "plt.ylim(max(depth2), min(depth2))\n",
        "plt.title('Pred DT Log 2: 15_9-F-11B', size=12)\n",
        "plt.xlabel('DT')\n",
        "plt.ylabel('Depth')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 8, 4)\n",
        "plt.plot(pad_predicted_values(y_pred2_lstm, depth2), depth2, color='cyan', label='LSTM')\n",
        "plt.ylim(max(depth2), min(depth2))\n",
        "plt.title('Pred DT Log 2: 15_9-F-11B', size=12)\n",
        "plt.xlabel('DT')\n",
        "plt.ylabel('Depth')\n",
        "plt.legend()\n",
        "\n",
        "# For log 5\n",
        "\n",
        "plt.subplot(1, 8, 5)\n",
        "plt.plot(y_pred5_decision_tree, depth5, color='green', label='Decision Tree')\n",
        "plt.ylim(max(depth5), min(depth5))\n",
        "plt.title('Pred DT Log 5: 15_9-F-1C', size=12)\n",
        "plt.xlabel('DT')\n",
        "plt.ylabel('Depth')\n",
        "plt.legend()\n",
        "\n",
        "\n",
        "plt.subplot(1, 8, 6)\n",
        "plt.plot(y_pred5_grad_boost, depth5, color='blue', label='Gradient Boosting')\n",
        "plt.ylim(max(depth5), min(depth5))\n",
        "plt.title('Pred DT Log 5: 15_9-F-1C', size=12)\n",
        "plt.xlabel('DT')\n",
        "plt.ylabel('Depth')\n",
        "plt.legend()\n",
        "\n",
        "\n",
        "plt.subplot(1, 8, 7)\n",
        "plt.plot(y_pred5_nn, depth5, color='orange', label='Neural Network')\n",
        "plt.ylim(max(depth5), min(depth5))\n",
        "plt.title('Pred DT Log 5: 15_9-F-1C', size=12)\n",
        "plt.xlabel('DT')\n",
        "plt.ylabel('Depth')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 8, 8)\n",
        "plt.plot(pad_predicted_values(y_pred5_lstm, depth5), depth5, color='cyan', label='LSTM')\n",
        "plt.ylim(max(depth5), min(depth5))\n",
        "plt.title('Pred DT Log 5: 15_9-F-1C', size=12)\n",
        "plt.xlabel('DT')\n",
        "plt.ylabel('Depth')\n",
        "plt.legend()\n",
        "\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "j0UjRm5O5DyH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def makeplotpred(df, color_list, suptitle_str=\"Pass a suptitle\"):\n",
        "    # Column selection from df\n",
        "    col_names = ['NPHI', 'RHOB', 'GR', 'RT', 'PEF', 'CALI', 'DT_Tree', 'DT_grad_boost', 'DT_nn', 'DT_lstm']\n",
        "    # Plotting titles\n",
        "    title = ['NPHI', 'RHOB', 'GR', 'RT', 'PEF', 'CALI', 'Predicted DT Decsion Tree', 'Predicted DT GB', 'Predicted DT NN', 'Predicted DT LSTM']\n",
        "\n",
        "    # Create the subplots; ncols equals the number of logs\n",
        "    fig, ax = plt.subplots(nrows=1, ncols=len(col_names), figsize=(20,15))\n",
        "    fig.suptitle(suptitle_str, size=20, y=1.05)\n",
        "\n",
        "    # Looping each log to display in the subplots\n",
        "    for i in range(len(col_names)):\n",
        "        if i == 3:\n",
        "            # for resistivity, semilog plot\n",
        "            ax[i].semilogx(df[col_names[i]], df['DEPTH'], color=color_list[i])\n",
        "\n",
        "        else:\n",
        "            # for non-resistivity, normal plot\n",
        "            ax[i].plot(df[col_names[i]], df['DEPTH'], color=color_list[i])\n",
        "\n",
        "        ax[i].set_ylim(max(df['DEPTH']), min(df['DEPTH']))\n",
        "        ax[i].set_title(title[i], pad=15)\n",
        "        ax[i].grid(True)\n",
        "        ax[i].legend()\n",
        "\n",
        "    ax[2].set_xlim(0, 200)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "mD-S3Nnp5GOP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming `depth` is the depth information shared among all logs\n",
        "makeplotpred(log2,\n",
        "             ['purple', 'purple', 'purple', 'purple', 'purple', 'purple',  'blue', 'green',  'orange', 'cyan'],\n",
        "             suptitle_str=\"Predictions for Log 2: 15_9-F-11B\")"
      ],
      "metadata": {
        "id": "GmmIUYHJ5Iry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "makeplotpred(log5,\n",
        "             ['purple', 'purple', 'purple', 'purple', 'purple', 'purple',  'blue',  'green', 'orange', 'cyan'],\n",
        "             suptitle_str=\"Predictions for Log 5: 15_9-F-1C\")"
      ],
      "metadata": {
        "id": "9geCC7yh5Kdf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5ZU-mQYq6myH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}